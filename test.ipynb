{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from landmarks import holostic_model\n",
    "from preprocess import load_data,get_interesting_idx\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Label_encoder_19.pkl\",'rb') as file:\n",
    "    label_encode = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"20_action_v5_acc_98_valacc_86\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_model = holostic_model()\n",
    "idx = get_interesting_idx()\n",
    "data_load = load_data(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signs = os.listdir(os.path.join(\"Real example signs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 262ms/step\n",
      "Acttual sign : After.mp4 Predicted sign : [['thankyou']] confidence : 52.95 %\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Acttual sign : All.mp4 Predicted sign : [['happy']] confidence : 87.31 %\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Acttual sign : Apple.mp4 Predicted sign : [['dad']] confidence : 91.49 %\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Acttual sign : Bad.mp4 Predicted sign : [['thankyou']] confidence : 97.67 %\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Acttual sign : Car.mp4 Predicted sign : [['sleepy']] confidence : 99.92 %\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Acttual sign : Dad.mp4 Predicted sign : [['dad']] confidence : 100.0 %\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Acttual sign : Face.mp4 Predicted sign : [['face']] confidence : 45.54 %\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Acttual sign : Go.mp4 Predicted sign : [['go']] confidence : 73.49 %\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Acttual sign : Happy_1.mp4 Predicted sign : [['happy']] confidence : 99.99 %\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Acttual sign : Hello_1.mp4 Predicted sign : [['hello']] confidence : 100.0 %\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Acttual sign : Like.mp4 Predicted sign : [['happy']] confidence : 56.06 %\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Acttual sign : Mom.mp4 Predicted sign : [['mom']] confidence : 96.02 %\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Acttual sign : Thank you.mp4 Predicted sign : [['thankyou']] confidence : 95.93 %\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Acttual sign : Wait.mp4 Predicted sign : [['all']] confidence : 99.59 %\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Acttual sign : Who.mp4 Predicted sign : [['face']] confidence : 45.54 %\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Acttual sign : Why.mp4 Predicted sign : [['why']] confidence : 80.47 %\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Acttual sign : Yes.mp4 Predicted sign : [['thankyou']] confidence : 75.66 %\n"
     ]
    }
   ],
   "source": [
    "# for sign in signs:\n",
    "#     value = mp_model.extract_values(os.path.join(\"Real example signs\",sign))\n",
    "#     value = data_load.load_no_sign_data(value)\n",
    "#     pred = model.predict(tf.expand_dims(value,0))\n",
    "#     confidence = round((pred[0][np.argmax(pred.squeeze())])*100,2)\n",
    "#     pred = label_encode.inverse_transform([[np.argmax(pred.squeeze())]])\n",
    "#     print(f\"Acttual sign : {sign} Predicted sign : {pred} confidence : {confidence} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time model prediction development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_holistic=mp.solutions.holistic\n",
    "\n",
    "def detection_model(img,model):\n",
    "        img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img.flags.writeable=False\n",
    "        result=model.process(img)\n",
    "        img.flags.writeable=True\n",
    "        img=cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return img,result\n",
    "    \n",
    "def draw_style(img,result):\n",
    "    #face landmark\n",
    "    mp_drawing.draw_landmarks(img, result.face_landmarks,mp_holistic.FACEMESH_CONTOURS,\n",
    "                                    mp_drawing.DrawingSpec(color=(220, 103, 112),thickness=1,circle_radius=1),\n",
    "                                    mp_drawing.DrawingSpec(color=(149, 234, 149),thickness=2,circle_radius=1))\n",
    "\n",
    "    #pose landmark\n",
    "    mp_drawing.draw_landmarks(img, result.pose_landmarks,mp_holistic.POSE_CONNECTIONS,\n",
    "                            mp_drawing.DrawingSpec(color=(20, 19, 247),thickness=2,circle_radius=2),\n",
    "                            mp_drawing.DrawingSpec(color=(149, 234, 149),thickness=2,circle_radius=1))\n",
    "    #left hand landmark\n",
    "    mp_drawing.draw_landmarks(img, result.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                            mp_drawing.DrawingSpec(color=(169, 42, 27),thickness=2,circle_radius=2),\n",
    "                            mp_drawing.DrawingSpec(color=(254, 114, 236),thickness=2,circle_radius=1))\n",
    "    #Right habd landmark\n",
    "    mp_drawing.draw_landmarks(img, result.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                            mp_drawing.DrawingSpec(color=(254, 114, 236),thickness=2,circle_radius=2),\n",
    "                            mp_drawing.DrawingSpec(color=(169, 42, 27),thickness=2,circle_radius=1))\n",
    "    \n",
    "\n",
    "def key_values(results):\n",
    "        pose = np.array([[result.x,result.y,result.z] for result in results.pose_landmarks.landmark]) if results.pose_landmarks else np.zeros((33,3)) \n",
    "        face = np.array([[result.x,result.y,result.z] for result in results.face_landmarks.landmark]) if results.face_landmarks else np.zeros((468,3))\n",
    "        right_hand = np.array([[result.x,result.y,result.y] for  result in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros((21,3))\n",
    "        left_hand = np.array([[result.x,result.y,result.z] for result in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros((21,3))\n",
    "        return np.concatenate([face,left_hand,pose,right_hand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "# Definig a container to hold the frames for prediction\n",
    "seq = []\n",
    "# Defining variable to hold the predicted signs\n",
    "sign =[]\n",
    "\n",
    "hand = []\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "#calling mediapipe line model and as to holistic variable\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.7) as holistic:\n",
    "\n",
    "#capturing frames and looping to show as video\n",
    "    while cap.isOpened():\n",
    "        ret, frame=cap.read() # ret will be bool (when runing it will be true)\n",
    "        # to strat while loop and below else is to break the loop\n",
    "        if ret == True:\n",
    "            #Converting to rgb and unwriteable to better with mediapipe and reversing and output image and result of model\n",
    "            img,result=detection_model(frame,holistic)\n",
    "\n",
    "            #drawing and extracting the result in the image \n",
    "            draw_style(img,result)\n",
    "            seq.append(key_values(result))\n",
    "\n",
    "            # to mirror the output image parameter 1 is use to flip along the y axix.\n",
    "            image = cv2.flip(frame,1)\n",
    "\n",
    "            # logic to check hand is detected in previous frames\n",
    "            if result.right_hand_landmarks or result.left_hand_landmarks:\n",
    "                hand.append(1)\n",
    "            else:\n",
    "                hand.append(0)\n",
    "            \n",
    "            # making the prediction for every 30 frames\n",
    "            if len(seq)%40 == 0 and sum(hand[-40:]) >10 :\n",
    "                value = np.array(seq[-30:]).squeeze()\n",
    "                value = data_load.load_no_sign_data(value)\n",
    "                pred = model.predict(tf.expand_dims(value,0))\n",
    "                confidence = round((pred[0][np.argmax(pred.squeeze())])*100,2)\n",
    "                pred = label_encode.inverse_transform([[np.argmax(pred.squeeze())]])\n",
    "                sign.append(pred[0][0])\n",
    "                seq = []\n",
    "            try:\n",
    "                if confidence > 95 and len(sign) >0:\n",
    "                    cv2.putText(image,' '.join(sign),(350,150),cv2.FONT_HERSHEY_TRIPLEX,1,(41, 172, 252),2)\n",
    "                    confidence = 0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                if confidence < 95 and len(sign) >0:\n",
    "                    cv2.putText(image,' '.join(sign[-3:]),(50,50),cv2.FONT_HERSHEY_TRIPLEX,1,(41, 172, 252),2)\n",
    "                    confidence = 0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            # displaying the resluted image\n",
    "            cv2.imshow('Capturing',image)\n",
    "\n",
    "            if cv2.waitKey(25) & 0xff == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
